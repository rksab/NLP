{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyTYbyIPAax4KJIJsL0F7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rksab/NLP/blob/main/Hugging_face_Course_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning landscape is constantly evolvingâ€”but rarely has a shift felt as sweeping as the rise of large language models. LLMs havenâ€™t just advanced the field; theyâ€™ve dominated it, capturing the imagination of both researchers and the general public in unprecedented ways.\n",
        "\n",
        "Traditionally, NLP models were narrow in scopeâ€”trained from scratch for specific tasks like sentiment analysis, translation, or named entity recognition. Each task required its own dataset, its own architecture, its own evaluation pipeline.\n",
        "\n",
        "That changed with the introduction of transformer architectures and large-scale pretraining. Together, they ushered in the era of LLMs.\n",
        "\n",
        "Instead of building task-specific models, researchers began training enormous modelsâ€”sometimes with hundreds of billions of parametersâ€”on massive corpora of text. These models learned general-purpose language understanding, which could later be fine-tuned or prompted to perform a wide variety of downstream tasks.\n",
        "\n",
        "It wasnâ€™t just an architectural shiftâ€”it was a shift in mindset.\n",
        "\n",
        "But do LLMs actually understand language? Not in the way humans do. They work on statistical patterns and are prone to hallucinations and bias. They require significant computational resources and text needs to be processed in a way that enables a model to learn from it."
      ],
      "metadata": {
        "id": "FyPV63t5xVGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Magic\n",
        "Let's start with some hugging face magic. We'll start with pipeline, as it assumes minimal knowledge to begin with."
      ],
      "metadata": {
        "id": "293pIrAiTrgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "kJbLR-ZbTQDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I love to hate this course\")"
      ],
      "metadata": {
        "id": "rUqw1N2uToYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we didn't supply a model, it picked up distilbert/distilbert-base-uncased-finetuned-sst-2-english. Under the hood, it tokenized the sentence, passed the tokens to the model and assigned it a label and score."
      ],
      "metadata": {
        "id": "8QNO5ViaT3WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "translator(\"Translate English to German: I love apples.\")"
      ],
      "metadata": {
        "id": "DRgn6jxtT2wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to perform a classification task without training aka zero-shot classification task."
      ],
      "metadata": {
        "id": "2Lqj9evnYuzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = pipeline(\"zero-shot-classification\")\n",
        "clf(\"This movie was so inspiring!\", candidate_labels=[\"sports\", \"politics\", \"entertainment\"])\n"
      ],
      "metadata": {
        "id": "aU1mc-CPY7ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "id": "e7Nvyy27Zm9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill = pipeline(\"fill-mask\")\n",
        "fill(\"The capital of France is <mask>.\", top_k = 2)"
      ],
      "metadata": {
        "id": "0WzB_buearkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## A Bit More About Models\n",
        "Weâ€™re about to dig into what happens behind the scenes in the deceptively simple pipeline() function. But first, letâ€™s take a moment to understand the different types of models it might use. There are three main architectural variants of Transformer models: encoder, decoder, and encoder-decoder.\n",
        "\n",
        "## Encoder Models\n",
        "Also called autoencoding models.\n",
        "\n",
        "The attention layers in encoder models have access to the entire input sequence at once (bidirectional attention).\n",
        "\n",
        "These models are useful when global understanding of the text is needed.\n",
        "\n",
        "They are typically trained to predict masked words, using both left and right context.\n",
        "\n",
        "Examples: bert-base-uncased, roberta-base\n",
        "\n",
        "Use cases: Text classification, sentence similarity, token classification (e.g., named entity recognition)\n",
        "\n",
        "## Decoder Models\n",
        "Also called autoregressive models.\n",
        "\n",
        "These models have access only to the tokens that come before the current one (unidirectional or causal attention).\n",
        "\n",
        "They are trained using causal language modeling, where the task is to predict the next token given the previous ones.\n",
        "\n",
        "Examples: gpt2, gpt-neo-125M\n",
        "\n",
        "Use cases: Text generation, code completion, chatbots\n",
        "\n",
        "##Encoder-Decoder Models\n",
        "Also called sequence-to-sequence models.\n",
        "\n",
        "The encoder reads the entire input, producing a hidden representation of it.\n",
        "\n",
        "The decoder then generates output one token at a time, using both the encoderâ€™s output and its own previously generated tokens.\n",
        "\n",
        "These models are often trained by corrupting the input and asking the model to reconstruct or translate it.\n",
        "\n",
        "Examples: t5-small, facebook/bart-large-cnn\n",
        "\n",
        "Use cases: Summarization, translation, question answering, and any task that involves generating one text based on another\n",
        "\n",
        "##What Makes These Models Effective?\n",
        "Attention.\n",
        "The attention mechanism allows the model to focus on the most relevant parts of the input while making predictions. Whether it's understanding the context of a word in a sentence or generating the next token in a sequence, attention plays a central role in making Transformers so powerful.\n"
      ],
      "metadata": {
        "id": "xCld_r0VdZHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "âœ¨ The First Magical Part of pipeline():\n",
        "The first thing that makes the pipeline() function so magical is the tokenizer.\n",
        "\n",
        "Models donâ€™t understand words â€” they understand numbers. So to perform any computational task, we first need to convert text into numbers. Thatâ€™s where tokenization comes in.\n",
        "\n",
        "\n",
        "ðŸ”¹ What happens during tokenization?\n",
        "We split the sentence into smaller parts, called tokens.\n",
        "\n",
        "Then, we map those tokens to unique integers using the modelâ€™s vocabulary.\n",
        "\n",
        "Thereâ€™s also some additional processing, like adding special tokens to mark the beginning and end of a sentence (for example, [CLS] and [SEP] in BERT).\n"
      ],
      "metadata": {
        "id": "fP_1Rz9V7f2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For tokenzation, hugging face has AutoTokenizer class and its from_pretrained() method. When no checkpoint is specified, pipeline uses the default one as we saw above."
      ],
      "metadata": {
        "id": "cE7J_6HAAjzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ],
      "metadata": {
        "id": "wTW5jTC5Bu24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code fetches the data associated with the tokenizer and caches it. Let's take a toy sentence"
      ],
      "metadata": {
        "id": "psiwF8juosCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['apples are sour. ']\n",
        "tokens = tokenizer(sentence)"
      ],
      "metadata": {
        "id": "CVx-3nMjorhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "iWxUVCODpsl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we want multiple sentences."
      ],
      "metadata": {
        "id": "-3AaJybPqGPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['apples are sour. ', 'We have promises to keep.']\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "hMc27-RpqFnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Feedforward Neural Networks (FFNNs), the input size must be fixed. However, in natural language processing, input sentences can vary in length. To handle this variability, we typically use padding and truncation.\n",
        "\n",
        "For models like RNNs or Transformers, we pad shorter sentences with special tokens (like [PAD]) so that all sentences in a batch have the same length. We may also truncate longer sentences to a specified maximum length to fit within model constraints or memory limits.\n",
        "\n",
        "Most modern language models define their own maximum input length (e.g., 512 tokens for BERT). Therefore, we often rely on the tokenizer to handle padding and truncation automatically, ensuring inputs are formatted correctly for the model. return_tensors specifies the type of tensor you want to get back."
      ],
      "metadata": {
        "id": "z83S5OrdqxmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoModel\n",
        "No need to dive deep yet â€” weâ€™ll explore internals of tokenizer later. For now, letâ€™s take a look at the next component of pipeline magic: AutoModel.\n",
        "\n",
        "Weâ€™ll start with AutoModel. It gives us access to the base model, which returns raw hidden states (not final predictions). Itâ€™s useful when you want to build custom heads on top or just understand whatâ€™s going on inside.\n",
        "\n",
        "Letâ€™s see it in action."
      ],
      "metadata": {
        "id": "DiAQpLFpruai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "q84yP2_Wrt6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**tokens)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "noHIFnaXsnIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models are large. Our output is of the size (batch_size, sequence length, hidden size). So we gave it tokens with max_size 8 and got a higher dimensional output. Our batch size was 2 as we just had 2 sentences. And 768 is embedding size, more abt that later.\n",
        "\n",
        "What can we do with the outputs? Say we turn it into a classification task. We want to label sentences as positive and negative. We need classification head for that. As you guessed, we'll use hugging face magic."
      ],
      "metadata": {
        "id": "a_Tzbs20stiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**tokens)"
      ],
      "metadata": {
        "id": "PlDsc58Vss_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "FahGSpva1TWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've logits. Let's interpret them and convert them into probabilities. We need a softmax layer."
      ],
      "metadata": {
        "id": "iuSOQXS02YGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "YsSnlZl31ddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label\n"
      ],
      "metadata": {
        "id": "tGWy6An13AA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First sentence is negative and second one is positive."
      ],
      "metadata": {
        "id": "4AzKtKyo3FQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P6AQvvOGstIA"
      }
    }
  ]
}